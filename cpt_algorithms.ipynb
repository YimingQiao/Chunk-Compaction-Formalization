{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374c044918092d9c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compaction Project\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Given a set of chunks whose sizes are\n",
    "\n",
    "$$\n",
    "S_n = \\{d_1, \\cdots, d_n\\},\n",
    "$$\n",
    "\n",
    "where the positive integer $d_i \\leq 2048$ for all $i = 1, \\cdots, n$. Suppose a pipeline containing $k$ join operators need time $\\mathcal{F}_k(d_i)$ to process a data chunk with the size $d_i$. \n",
    "\n",
    "Our goal is to compact the set $S$, i.e., we need a transformation\n",
    "\n",
    "$$\n",
    "\\mathcal{M}: S_n \\rightarrow S'_m \\triangleq \\{d'_1,  \\cdots, d'_m\\},\n",
    "$$\n",
    "\n",
    "where $\\sum_i^n d_i = \\sum_j^m d'_j$ and $m$ is an arbitrary integer less than $n$, to minimize \n",
    "\n",
    "$$\n",
    "\\sum_j^m \\mathcal{F}(d'_j) + cost(M, S).\n",
    "$$\n",
    "\n",
    "where $cost(\\mathcal{M}, S)$ is the cost of the transformation $\\mathcal{M}$ on the set $S$. The cost of combining two or more chunks into one: $d_i + \\cdots + d_j = d'_s \\leq 2048$, is \n",
    "\n",
    "$$\n",
    "g(d'_s) = C_3 + d'_s \\times C_4.\n",
    "$$\n",
    "\n",
    "**Note:** This formulated problem is easier than the real compaction problem because we know sizes of all chunks in advance, as opposed to dealing with a stream of chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364336a3",
   "metadata": {},
   "source": [
    "## Cost Calculation\n",
    "\n",
    "Suppose the $i$-th join operator needs time $f_i(d) = C_1 + C_2 \\cdot d$ to process a data chunk with the size $d$. \n",
    "\n",
    "Suppose the pipeline has $k$ join operators, we have two assumptions:\n",
    "\n",
    "**Assumption 1**: Each join receives a data chunk, and outputs $m$ smaller chunks. \n",
    "\n",
    "**Assumption 2**: The total number of tuples across the $m$ smaller chunks remains the same as the number of tuples in the original input chunk. \n",
    "\n",
    "Then, for an input chunk with the size $d$, the needed execution time $\\mathcal{F}_k (d)$ consists of two parts: \n",
    "\n",
    "1. Per Tuple Cost \n",
    "\n",
    "$$ C_2 \\cdot d \\cdot k $$ \n",
    "\n",
    "2. Fixed Cost\n",
    "\n",
    "$$ C_1 \\cdot \\min\\{m^0, d\\} + C_1 \\cdot \\min\\{m^1, d\\} + \\cdots + C_1 \\cdot \\min\\{m^{k-1}, d\\}$$\n",
    "\n",
    "We need to take the minimum value betwwen $m^{i}$ and the $d$ because a data chunk cannot be split into more than $d$ smaller chunks. In other words, each chunk has at least one tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9897d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from termcolor import colored\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# settings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def print_color(text, color=None):\n",
    "    if color:\n",
    "        print(colored(text, color))\n",
    "    else:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50c30d",
   "metadata": {},
   "source": [
    "## Query Execution Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f7c1da65ee0c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate random chunk sizes from a Gaussian distribution\n",
    "def generate_chunk_sizes(n, mean=64, scale=256, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    return np.minimum(2048, np.maximum(1, np.random.normal(mean, scale, n))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simulation of Joining and Compacting\n",
    "\n",
    "#             fixed cost      per tuple cost\n",
    "# probe()     1.5             0.03\n",
    "# next()      0.9             0.06\n",
    "# --------------------------------------\n",
    "# compact()   0.3             0.05\n",
    "# --------------------------------------\n",
    "\n",
    "k_prc_fixed_cost = (1.5 + 0.9)\n",
    "k_prc_per_tuple_cost = (0.03 + 0.06)\n",
    "k_cpt_fixed_cost = 0.3\n",
    "k_cpt_per_tuple_cost = 0.05\n",
    "\n",
    "# This function split each chunk into smaller chunks.\n",
    "def split_array_np(numbers, parts):\n",
    "    numbers = np.array(numbers)\n",
    "\n",
    "    quotient, remainder = np.divmod(numbers, parts)\n",
    "    split = np.repeat(quotient, parts)\n",
    "    remainder = np.repeat(remainder, parts)\n",
    "    split[np.arange(numbers.size * parts) % parts < remainder] += 1\n",
    "        \n",
    "    split = split[split > 0]\n",
    "\n",
    "    return split\n",
    "\n",
    "# Simulate the execution of joins.\n",
    "def simulate_join(sizes, compact_func, chunk_factor=1, n_join=1, print_log=True):\n",
    "    prc_cost = 0\n",
    "    cpt_cost = 0\n",
    "    next_sizes = np.array(sizes)\n",
    "\n",
    "    if print_log:\n",
    "        print_color(f\"-------------------------\", 'green')\n",
    "        print_color(f\"Compactor {compact_func}\", 'green')\n",
    "\n",
    "    for l in reversed(range(n_join)):\n",
    "        # 1. Join\n",
    "        prc_cost += k_prc_fixed_cost * len(next_sizes) + np.sum(next_sizes) * k_prc_per_tuple_cost\n",
    "\n",
    "        # 2. Split chunks\n",
    "        next_sizes = split_array_np(next_sizes, chunk_factor)\n",
    "        input_chunk_number = len(next_sizes)\n",
    "\n",
    "        # 3. Compact\n",
    "        next_sizes, cost = compact_func(next_sizes, chunk_factor, l)\n",
    "        output_chunk_number = len(next_sizes)\n",
    "\n",
    "        if print_log:\n",
    "            print_color(f\"Level {n_join - l}: {input_chunk_number} -> {output_chunk_number} chunks, cost: {cost:.2f}\", 'green')\n",
    "\n",
    "        cpt_cost += cost\n",
    "\n",
    "    return prc_cost, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b962e0f",
   "metadata": {},
   "source": [
    "## Compaction Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions of computing join and compaction cost\n",
    "def compute_prc_cost(chunk_size, chunk_factor, n_joins):\n",
    "    per_tuple_cost = k_prc_per_tuple_cost * chunk_size * n_joins\n",
    "\n",
    "    fixed_cost = 0\n",
    "    for i in range(n_joins):\n",
    "        fixed_cost += k_prc_fixed_cost * np.minimum(chunk_factor ** i, chunk_size)\n",
    "    \n",
    "    # print(f\"per_tuple_cost: {per_tuple_cost:.2f}, fixed_cost: {fixed_cost:.2f}\")\n",
    "    return per_tuple_cost + fixed_cost\n",
    "\n",
    "\n",
    "# Compute the cost of a single compaction\n",
    "def compute_cpt_cost(sizes_in_one_compaction):\n",
    "    return k_cpt_fixed_cost + np.sum(sizes_in_one_compaction) * k_cpt_per_tuple_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b05aa4",
   "metadata": {},
   "source": [
    "### Base Strategies\n",
    "\n",
    "**Strategy 1**: Do not compact any chunks. \n",
    "\n",
    "**Strategy 2**: Fully compact all chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_no_compaction(chunk_sizes, chunk_factor, level):\n",
    "    return chunk_sizes, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_full_compaction(chunk_sizes, chunk_factor, level):\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "    cpt_sizes = []\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        if size == 2048: \n",
    "            transformed_sizes.append(size)\n",
    "            continue\n",
    "\n",
    "        if sum(cpt_sizes) + size <= 2048:\n",
    "            cpt_sizes.append(size)\n",
    "        else:\n",
    "            l_size = 2048 - sum(cpt_sizes)\n",
    "            cpt_sizes.append(l_size)\n",
    "\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "            cpt_sizes = [size - l_size]\n",
    "    \n",
    "    if cpt_sizes:\n",
    "        cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "        transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0030d",
   "metadata": {},
   "source": [
    "### Optimal Strategy\n",
    "\n",
    "**Strategy 3**: Sort all chunks ascendingly. For each compaction, it contains small chunks as more as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_sort_compaction(chunk_sizes, chunk_factor, n_joins):\n",
    "    assert len(chunk_sizes) > 0, \"chunk_sizes must not be empty\"\n",
    "\n",
    "    sorted_sizes = sorted(chunk_sizes)\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "\n",
    "    i = 0\n",
    "    cpt_sizes = [sorted_sizes[0]]\n",
    "    for i in range(1, len(sorted_sizes), 1):\n",
    "        size = sorted_sizes[i]\n",
    "        cur_sum = sum(cpt_sizes)\n",
    "\n",
    "        if cur_sum + size <= 2048:\n",
    "            gain = compute_prc_cost(cur_sum, chunk_factor, n_joins) + compute_prc_cost(size, chunk_factor, n_joins) - compute_prc_cost(cur_sum + size, chunk_factor, n_joins)\n",
    "            loss = (k_cpt_fixed_cost + size * k_cpt_per_tuple_cost)\n",
    "            \n",
    "            # print(f\"gain: {gain:.2f}, loss: {loss:.2f}\")\n",
    "            # print(f\"cur sum {cur_sum} + size {size} = {cur_sum + size}\")\n",
    "\n",
    "            if gain - loss > 0:\n",
    "                cpt_sizes.append(size)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            if len(cpt_sizes) > 1:\n",
    "                cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "            cpt_sizes = [size]\n",
    "\n",
    "    if cpt_sizes:\n",
    "        if len(cpt_sizes) > 1:\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "        transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "    # Append the remaining chunks, it is not beneficial to compact them\n",
    "    for j in range(i+1, len(sorted_sizes)):\n",
    "        transformed_sizes.append(sorted_sizes[j])\n",
    "        \n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9407bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_benefit(buffer, target, chunk_factor, n_joins):\n",
    "    prc_gain = compute_prc_cost(buffer, chunk_factor, n_joins) + compute_prc_cost(target, chunk_factor, n_joins) - compute_prc_cost(buffer + target, chunk_factor, n_joins)\n",
    "    cpt_cost = (k_cpt_fixed_cost + target * k_cpt_per_tuple_cost)\n",
    "    return prc_gain - cpt_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compaction threshold is necessary\n",
    "\n",
    "chunk_factor = 4\n",
    "n_joins = 2\n",
    "x_base = np.arange(1, 2048, 1)\n",
    "y_add = np.arange(1, 2048, 1)\n",
    "\n",
    "z_benefits = np.zeros((2047, 2047))\n",
    "for i in range(2047):\n",
    "    for j in range(2047):\n",
    "        if x_base[i] + y_add[j] <= 2048 and x_base[i] + y_add[j] > 0:\n",
    "            z_benefits[j][i] = compute_benefit(x_base[i], y_add[j], chunk_factor, n_joins)\n",
    "        else:\n",
    "            z_benefits[j][i] = None\n",
    "\n",
    "# Assuming z_benefits is a numpy array\n",
    "y_zero, x_zero = np.where(np.abs(z_benefits) < 0.01)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(x=x_base, y=y_add, z=z_benefits, colorscale='ice', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_base[x_zero], y=y_add[y_zero], mode='lines', name='z = 0', line=dict(color='black',width=2)))\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=500,\n",
    "    height=500,\n",
    "    title=\"Compaction Benefit\",\n",
    "    xaxis_title=\"Buffer Chunks Size\",\n",
    "    yaxis_title=\"Target Chunk Size\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b707b",
   "metadata": {},
   "source": [
    "~~**Strategy 3.5**: Sort all chunks ascendingly. For each small chunk, distribute it into the largers.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_sort_compaction_one_hop(chunk_sizes, chunk_factor, n_joins):\n",
    "    assert len(chunk_sizes) > 0, \"chunk_sizes must not be empty\"\n",
    "\n",
    "    sorted_sizes = sorted(chunk_sizes)\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "\n",
    "    def is_small_chunk(size, n_joins):\n",
    "        return k_prc_fixed_cost * (n_joins > 0) > k_cpt_fixed_cost + size * k_cpt_per_tuple_cost\n",
    "\n",
    "    i = 0\n",
    "    for i in range(0, len(sorted_sizes)):\n",
    "        size = sorted_sizes[i]\n",
    "        assert size != 0, \"size must not be zero\"\n",
    "        \n",
    "        if not is_small_chunk(size, n_joins):\n",
    "            break\n",
    "\n",
    "        # check if this chunk can be distributed into other chunks\n",
    "        n_slot = 0\n",
    "        j = 0\n",
    "        for j in range(len(sorted_sizes) - 1, i, -1):\n",
    "            n_slot += 2048 - sorted_sizes[j]\n",
    "            if n_slot >= size: break\n",
    "\n",
    "        if n_slot < size:\n",
    "            break;\n",
    "\n",
    "        # compute the compaction cost\n",
    "        cpt_cost += compute_cpt_cost([size])\n",
    "\n",
    "        # distribute this chunk into other chunks\n",
    "        assert j+1 <= len(sorted_sizes), \"j+1 must be less than len(sorted_sizes)\"\n",
    "        k = 0\n",
    "        for k in range(j+1, len(sorted_sizes)): \n",
    "            size -= 2048 - sorted_sizes[k]\n",
    "            sorted_sizes[k] = 2048\n",
    "\n",
    "        sorted_sizes[j] += size\n",
    "\n",
    "    # Append the remaining chunks, it is not beneficial to compact them\n",
    "    for j in range(i, len(sorted_sizes)):\n",
    "        transformed_sizes.append(sorted_sizes[j])\n",
    "\n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1444548",
   "metadata": {},
   "source": [
    "### DuckDB Strategy\n",
    "\n",
    "**Strategy 4**: Set a threshold to distinguish the big chunk and the small chunk, we only compact small chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bace28",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_block_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_binary_compaction(chunk_sizes, chunk_factor, level, threshold=128):\n",
    "    trans_chunks = []\n",
    "    cpt_cost = 0\n",
    "    \n",
    "    cpt_chunks = []\n",
    "    for size in chunk_sizes:\n",
    "        if size >= threshold:\n",
    "            trans_chunks.append(size)\n",
    "            continue\n",
    "\n",
    "        cpt_chunks.append(size)\n",
    "\n",
    "        if sum(cpt_chunks) >= k_block_size - threshold:\n",
    "            trans_chunks.append(np.sum(cpt_chunks))\n",
    "            cpt_cost += compute_cpt_cost(cpt_chunks)\n",
    "            cpt_chunks = []\n",
    "    \n",
    "    return trans_chunks, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb8b97",
   "metadata": {},
   "source": [
    "**Strategy 5**: The last strategy uses fixed threshold 128 for all joins. It is not precise, and we can compute a better threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpt_threhold(chunk_factor, n_joins):\n",
    "    if n_joins == 0:\n",
    "        return 0\n",
    "\n",
    "    x = 1024\n",
    "    y = np.arange(1, 1025, 1)\n",
    "    z = np.zeros(1024)\n",
    "\n",
    "    for i in range(1024):\n",
    "        if x + y[i] <= 2048 and x + y[i] > 0:\n",
    "            z[i] = compute_benefit(x, y[i], chunk_factor, n_joins)\n",
    "        else:\n",
    "            z[i] = None\n",
    "\n",
    "    positive_indices = np.where(z > 0)[0]\n",
    "    last_positive_index = positive_indices[-1] + 1 if len(positive_indices) > 0 else 0\n",
    "\n",
    "    return last_positive_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_dynamic_compaction(chunk_sizes, chunk_factor, level):\n",
    "    cpt_threshold = get_cpt_threhold(chunk_factor, level)\n",
    "\n",
    "    print_color(f\"Compaction Threshold: {cpt_threshold}\")\n",
    "\n",
    "    trans_chunks = []\n",
    "    cpt_cost = 0\n",
    "    \n",
    "    cpt_chunks = []\n",
    "    for size in chunk_sizes:\n",
    "        if size >= cpt_threshold:\n",
    "            trans_chunks.append(size)\n",
    "            continue\n",
    "\n",
    "        cpt_chunks.append(size)\n",
    "\n",
    "        if sum(cpt_chunks) >= k_block_size - cpt_threshold:\n",
    "            trans_chunks.append(np.sum(cpt_chunks))\n",
    "            cpt_cost += compute_cpt_cost(cpt_chunks)\n",
    "            cpt_chunks = []\n",
    "\n",
    "    \n",
    "    return trans_chunks, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e815e7",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f51c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def let_us_join(chunk_sizes, chunk_factor, num_join, print_log=True):\n",
    "    print_color(f\"1. Settings: \\n\", 'red')\n",
    "    print_color(f\"\\t\\t\\t Join\\t Compaction\")\n",
    "    print_color(f\"Fixed Cost (us) \\t {k_prc_fixed_cost}\\t {k_cpt_fixed_cost}\")\n",
    "    print_color(f\"Per-tuple Cost (us) \\t {k_prc_per_tuple_cost}\\t {k_cpt_per_tuple_cost}\")\n",
    "    print(\"\\t\")\n",
    "    print_color(f\"Chunk Factor: {chunk_factor}\\nNumber of Joins: {num_join}\")\n",
    "\n",
    "    print_color(f\"-------------------------\" * 3)\n",
    "    print_color(f\"2. Estimated Thresholds: \\n\", 'red')\n",
    "\n",
    "\n",
    "    grades = {\n",
    "        \"No Compaction\": simulate_join(chunk_sizes, alg_no_compaction, chunk_factor, num_join, print_log), \n",
    "        \"Full Compaction\": simulate_join(chunk_sizes, alg_full_compaction, chunk_factor, num_join, print_log),\n",
    "        \"Sort Compaction\": simulate_join(chunk_sizes, alg_sort_compaction, chunk_factor, num_join, print_log),\n",
    "        # \"Sort (One Hop) Compaction\": simulate_join(chunk_sizes, alg_sort_compaction_one_hop, chunk_factor, num_join, print_log),\n",
    "        \"Binary Compaction\": simulate_join(chunk_sizes, alg_binary_compaction, chunk_factor, num_join, print_log), \n",
    "        \"Dynamic Compaction\": simulate_join(chunk_sizes, alg_dynamic_compaction, chunk_factor, num_join, print_log),\n",
    "    }\n",
    "\n",
    "    print_color(f\"-------------------------\" * 3)\n",
    "    print_color(f\"3. Results: \\n\", 'red')\n",
    "\n",
    "    for grade in grades:\n",
    "        prc_cost = grades[grade][0]/1e6\n",
    "        cpt_cost = grades[grade][1]/1e6\n",
    "        print_color(f\"[{grade}]\", 'green')\n",
    "        print(f\"\\t Total Cost: {prc_cost + cpt_cost:.2f}s\\tCompute Cost: {prc_cost:.2f}s\\t Compaction Cost: {cpt_cost:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Chunk Distribution\n",
    "chunk_sizes = generate_chunk_sizes(n=int(2e7 / 2048), mean=2048, scale=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87205e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "\n",
    "chunk_factor = 16\n",
    "num_join = 3\n",
    "print_log = False\n",
    "\n",
    "let_us_join(chunk_sizes, chunk_factor, num_join, print_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Compaction Cost\n",
    "\n",
    "k_cpt_per_tuple_cost *= 10\n",
    "\n",
    "chunk_factor = 16\n",
    "num_join = 3\n",
    "print_log = False\n",
    "let_us_join(chunk_sizes, chunk_factor, num_join, print_log)\n",
    "\n",
    "k_cpt_per_tuple_cost /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Compaction Cost & Deep Pipeline\n",
    "\n",
    "k_cpt_per_tuple_cost *= 10\n",
    "\n",
    "chunk_factor = 12\n",
    "num_join = 4\n",
    "print_log = False\n",
    "let_us_join(chunk_sizes, chunk_factor, num_join, print_log)\n",
    "\n",
    "k_cpt_per_tuple_cost /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342a063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
