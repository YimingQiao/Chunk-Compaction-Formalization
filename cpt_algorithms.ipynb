{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374c044918092d9c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compaction Project\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Given a set of chunks whose sizes are\n",
    "\n",
    "$$\n",
    "S_n = \\{d_1, \\cdots, d_n\\},\n",
    "$$\n",
    "\n",
    "where the positive integer $d_i \\leq 2048$ for all $i = 1, \\cdots, n$. Suppose the remaining operators need time \n",
    "\n",
    "$$\n",
    "f(d_i) = C_1 + d_i \\times C_2\n",
    "$$\n",
    "\n",
    "to process a data chunk with the size $d_i$. Our goal is to compact the set $S$, i.e., we need a transformation\n",
    "\n",
    "$$\n",
    "\\mathcal{M}: S_n \\rightarrow S'_m \\triangleq \\{d'_1,  \\cdots, d'_m\\},\n",
    "$$\n",
    "\n",
    "where $\\sum_i^n d_i = \\sum_j^m d'_j$ and $m$ is an arbitrary integer less than $n$, to minimize \n",
    "\n",
    "$$\n",
    "\\sum_j^m f(d'_j) + cost(M, S).\n",
    "$$\n",
    "\n",
    "where $cost(\\mathcal{M}, S)$ is the cost of the transformation $\\mathcal{M}$ on the set $S$. \n",
    "\n",
    "The cost of combining two or more chunks into one: $d_i + \\cdots + d_j = d'_s \\leq 2048$, is \n",
    "\n",
    "$$\n",
    "g(d'_s) = C_3 + d'_s \\times C_4.\n",
    "$$\n",
    "\n",
    "**Note:** This formulated problem is easier than the real compaction problem because we have the sizes of all data chunks in advance, rather than a chunk stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1ae6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from termcolor import colored\n",
    "\n",
    "def print_color(text, color='black'):\n",
    "    print(colored(text, color))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50c30d",
   "metadata": {},
   "source": [
    "## 1. Chunk Sizes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c62f7c1da65ee0c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random chunk sizes from a Gaussian distribution\n",
    "def generate_chunk_sizes(n, mean=64, scale=256):\n",
    "    return np.minimum(2048, np.maximum(1, np.random.normal(mean, scale, n))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e3e7b",
   "metadata": {},
   "source": [
    "## 2. Compaction Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30b8dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             fixed cost      per tuple cost\n",
    "# probe()     1.5             0.03\n",
    "# next()      0.9             0.06\n",
    "# --------------------------------------\n",
    "# compact()   0.3             0.03\n",
    "# --------------------------------------\n",
    "\n",
    "k_prc_fixed_cost = (1.5 + 0.9)\n",
    "k_prc_per_tuple_cost = (0.03 + 0.06)\n",
    "k_cpt_fixed_cost = 0.3\n",
    "k_cpt_per_tuple_cost = 0.03\n",
    "\n",
    "# This function split each chunk into smaller chunks.\n",
    "def split_array_np(numbers, parts):\n",
    "    numbers = np.array(numbers)\n",
    "\n",
    "    quotient, remainder = np.divmod(numbers, parts)\n",
    "    split = np.repeat(quotient, parts)\n",
    "    remainder = np.repeat(remainder, parts)\n",
    "    split[np.arange(numbers.size * parts) % parts < remainder] += 1\n",
    "        \n",
    "    split = split[split > 0]\n",
    "\n",
    "    return split\n",
    "\n",
    "# Simulate the execution of joins.\n",
    "def simulate_join(sizes, compact_func, chunk_factor=1, n_join=1, print_log=True):\n",
    "    prc_cost = 0\n",
    "    cpt_cost = 0\n",
    "    next_sizes = np.array(sizes)\n",
    "\n",
    "    if print_log:\n",
    "        print_color(f\"-------------------------\", 'green')\n",
    "        print_color(f\"Compactor {compact_func}\", 'green')\n",
    "\n",
    "    for l in reversed(range(n_join)):\n",
    "        # 1. Join\n",
    "        prc_cost += k_prc_fixed_cost * len(next_sizes) + np.sum(next_sizes) * k_prc_per_tuple_cost\n",
    "\n",
    "        # 2. Split chunks\n",
    "        next_sizes = split_array_np(next_sizes, chunk_factor)\n",
    "        input_chunk_number = len(next_sizes)\n",
    "\n",
    "        # 3. Compact\n",
    "        next_sizes, cost = compact_func(next_sizes, chunk_factor, l)\n",
    "        output_chunk_number = len(next_sizes)\n",
    "\n",
    "        if print_log:\n",
    "            print_color(f\"Level {n_join - l}: {input_chunk_number} -> {output_chunk_number} chunks, cost: {cost:.2f}\", 'green')\n",
    "\n",
    "        cpt_cost += cost\n",
    "\n",
    "    return prc_cost, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b962e0f",
   "metadata": {},
   "source": [
    "## 3. Compaction Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afe82a",
   "metadata": {},
   "source": [
    "### Cost Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3e9113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prc_cost(chunk_size, chunk_factor, n_joins):\n",
    "    per_tuple_cost = k_prc_per_tuple_cost * chunk_size * n_joins\n",
    "\n",
    "    fixed_cost = 0\n",
    "    for i in range(n_joins):\n",
    "        fixed_cost += k_prc_fixed_cost * np.minimum(chunk_factor ** i, chunk_size)\n",
    "    \n",
    "    # print(f\"per_tuple_cost: {per_tuple_cost:.2f}, fixed_cost: {fixed_cost:.2f}\")\n",
    "    return per_tuple_cost + fixed_cost\n",
    "\n",
    "\n",
    "# Compute the cost of a single compaction\n",
    "def compute_cpt_cost(sizes_in_one_compaction):\n",
    "    return k_cpt_fixed_cost + np.sum(sizes_in_one_compaction) * k_cpt_per_tuple_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b05aa4",
   "metadata": {},
   "source": [
    "### Base Strategies\n",
    "\n",
    "**Strategy 1**: Do not compact any chunks.\n",
    "\n",
    "**Strategy 2**: Fully compact all chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb69b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_no_compaction(chunk_sizes, chunk_factor, level):\n",
    "    return chunk_sizes, 0\n",
    "\n",
    "\n",
    "def alg_full_compaction(chunk_sizes, chunk_factor, level):\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "    cpt_sizes = []\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        if size == 2048: \n",
    "            transformed_sizes.append(size)\n",
    "            continue\n",
    "\n",
    "        if sum(cpt_sizes) + size <= 2048:\n",
    "            cpt_sizes.append(size)\n",
    "        else:\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "            cpt_sizes = [size]\n",
    "    \n",
    "    if cpt_sizes:\n",
    "        cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "        transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0030d",
   "metadata": {},
   "source": [
    "### Optimal Strategy\n",
    "\n",
    "**Strategy 3**: Sort all chunks ascendingly and compact them in order, until compaction is not beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e214a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_sort_compaction(chunk_sizes, chunk_factor, n_joins):\n",
    "    assert len(chunk_sizes) > 0, \"chunk_sizes must not be empty\"\n",
    "\n",
    "    sorted_sizes = sorted(chunk_sizes)\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "\n",
    "    i = 0\n",
    "    cpt_sizes = [sorted_sizes[0]]\n",
    "    for i in range(1, len(sorted_sizes), 1):\n",
    "        size = sorted_sizes[i]\n",
    "        cur_sum = sum(cpt_sizes)\n",
    "\n",
    "        if cur_sum + size <= 2048:\n",
    "            gain = compute_prc_cost(cur_sum, chunk_factor, n_joins) + compute_prc_cost(size, chunk_factor, n_joins) - compute_prc_cost(cur_sum + size, chunk_factor, n_joins)\n",
    "            loss = (k_cpt_fixed_cost + size * k_cpt_per_tuple_cost)\n",
    "            \n",
    "            # print(f\"gain: {gain:.2f}, loss: {loss:.2f}\")\n",
    "            # print(f\"cur sum {cur_sum} + size {size} = {cur_sum + size}\")\n",
    "\n",
    "            if gain - loss > 0:\n",
    "                cpt_sizes.append(size)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            if len(cpt_sizes) > 1:\n",
    "                cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "            cpt_sizes = [size]\n",
    "\n",
    "    if cpt_sizes:\n",
    "        if len(cpt_sizes) > 1:\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "        transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "    # Append the remaining chunks, it is not beneficial to compact them\n",
    "    for j in range(i+1, len(sorted_sizes)):\n",
    "        transformed_sizes.append(sorted_sizes[j])\n",
    "        \n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1444548",
   "metadata": {},
   "source": [
    "### DuckDB Strategy\n",
    "\n",
    "**Strategy 4**: Set a threhold to distinguish the big chunk and the small chunk, we only compact small chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f169b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_block_size = 2048\n",
    "k_duckdb_compaction_threshold = 128\n",
    "\n",
    "def alg_binary_compaction(chunk_sizes, chunk_factor, level):\n",
    "    trans_chunks = []\n",
    "    cpt_cost = 0\n",
    "    \n",
    "    cpt_chunks = []\n",
    "    for size in chunk_sizes:\n",
    "        if size >= k_duckdb_compaction_threshold:\n",
    "            trans_chunks.append(size)\n",
    "            continue\n",
    "\n",
    "        cpt_chunks.append(size)\n",
    "\n",
    "        if sum(cpt_chunks) >= k_block_size - k_duckdb_compaction_threshold:\n",
    "            trans_chunks.append(np.sum(cpt_chunks))\n",
    "            cpt_cost += compute_cpt_cost(cpt_chunks)\n",
    "            cpt_chunks = []\n",
    "\n",
    "    \n",
    "    return trans_chunks, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e815e7",
   "metadata": {},
   "source": [
    "## 4. Let's Join!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87205e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m-------------------------\u001b[0m\n",
      "\u001b[32m[No Compaction]\u001b[0m\n",
      "\t Total Cost: 5.83s\tCompute Cost: 5.83s\t Compaction Cost: 0.00s\n",
      "\u001b[32m[Full Compaction]\u001b[0m\n",
      "\t Total Cost: 3.55s\tCompute Cost: 2.67s\t Compaction Cost: 0.88s\n",
      "\u001b[32m[Sort Compaction]\u001b[0m\n",
      "\t Total Cost: 3.13s\tCompute Cost: 2.84s\t Compaction Cost: 0.29s\n",
      "\u001b[32m[Binary Compaction]\u001b[0m\n",
      "\t Total Cost: 3.48s\tCompute Cost: 2.76s\t Compaction Cost: 0.72s\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = generate_chunk_sizes(n=int(1e7 / 2048), mean=2048, scale=128)\n",
    "chunk_factor = 16\n",
    "num_join = 3\n",
    "print_log = False\n",
    "\n",
    "grades = {\n",
    "    \"No Compaction\": simulate_join(chunk_sizes, alg_no_compaction, chunk_factor, num_join, print_log), \n",
    "    \"Full Compaction\": simulate_join(chunk_sizes, alg_full_compaction, chunk_factor, num_join, print_log),\n",
    "    \"Sort Compaction\": simulate_join(chunk_sizes, alg_sort_compaction, chunk_factor, num_join, print_log),\n",
    "    \"Binary Compaction\": simulate_join(chunk_sizes, alg_binary_compaction, chunk_factor, num_join, print_log), \n",
    "}\n",
    "\n",
    "print_color(f\"-------------------------\", 'green')\n",
    "\n",
    "for grade in grades:\n",
    "    prc_cost = grades[grade][0]/1e6\n",
    "    cpt_cost = grades[grade][1]/1e6\n",
    "    print_color(f\"[{grade}]\", 'green')\n",
    "    print(f\"\\t Total Cost: {prc_cost + cpt_cost:.2f}s\\tCompute Cost: {prc_cost:.2f}s\\t Compaction Cost: {cpt_cost:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8125f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b7cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
