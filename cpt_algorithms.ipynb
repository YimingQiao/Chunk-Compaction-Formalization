{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374c044918092d9c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compaction Project\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Given a set of chunks whose sizes are\n",
    "\n",
    "$$\n",
    "S_n = \\{d_1, \\cdots, d_n\\},\n",
    "$$\n",
    "\n",
    "where the positive integer $d_i \\leq 2048$ for all $i = 1, \\cdots, n$. Suppose the remaining operators need time \n",
    "\n",
    "$$\n",
    "f(d_i) = C_1 + d_i \\times C_2\n",
    "$$\n",
    "\n",
    "to process a data chunk with the size $d_i$. Our goal is to compact the set $S$, i.e., we need a transformation\n",
    "\n",
    "$$\n",
    "\\mathcal{M}: S_n \\rightarrow S'_m \\triangleq \\{d'_1,  \\cdots, d'_m\\},\n",
    "$$\n",
    "\n",
    "where $\\sum_i^n d_i = \\sum_j^m d'_j$ and $m$ is an arbitrary integer less than $n$, to minimize \n",
    "\n",
    "$$\n",
    "\\sum_j^m f(d'_j) + cost(M, S).\n",
    "$$\n",
    "\n",
    "where $cost(\\mathcal{M}, S)$ is the cost of the transformation $\\mathcal{M}$ on the set $S$. \n",
    "\n",
    "The cost of combining two or more chunks into one: $d_i + \\cdots + d_j = d'_s \\leq 2048$, is \n",
    "\n",
    "$$\n",
    "g(d'_s) = C_3 + d'_s \\times C_4.\n",
    "$$\n",
    "\n",
    "**Note:** This formulated problem is easier than the real compaction problem because we have the sizes of all data chunks in advance, rather than a chunk stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1ae6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from termcolor import colored\n",
    "\n",
    "def print_color(text, color='black'):\n",
    "    print(colored(text, color))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50c30d",
   "metadata": {},
   "source": [
    "## 1. Chunk Sizes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c62f7c1da65ee0c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random chunk sizes from a Gaussian distribution\n",
    "def generate_chunk_sizes(n, mean=64, scale=256):\n",
    "    return np.minimum(2048, np.maximum(1, np.random.normal(mean, scale, n))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e3e7b",
   "metadata": {},
   "source": [
    "## 2. Compaction Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "30b8dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             fixed cost      per tuple cost\n",
    "# probe()     1.5             0.03\n",
    "# next()      0.9             0.06\n",
    "# --------------------------------------\n",
    "# compact()   0.3             0.03\n",
    "# --------------------------------------\n",
    "\n",
    "k_pcs_fixed_cost = (1.5 + 0.9)\n",
    "k_pcs_per_tuple_cost = (0.03 + 0.06)\n",
    "k_cpt_fixed_cost = 0.3\n",
    "k_cpt_per_tuple_cost = 0.03\n",
    "\n",
    "def simulate_join(sizes, compact_func, chunk_factor=1, level=1):\n",
    "    prc_cost = 0\n",
    "    cpt_cost = 0\n",
    "    next_sizes = np.array(sizes)\n",
    "\n",
    "    for _ in range(level):\n",
    "        # join\n",
    "        prc_cost += k_pcs_fixed_cost * len(next_sizes) + np.sum(next_sizes) * k_pcs_per_tuple_cost\n",
    "        next_sizes = np.repeat(np.array(next_sizes) // chunk_factor, chunk_factor)\n",
    "\n",
    "        # compact\n",
    "        next_sizes, cost = compact_func(next_sizes, chunk_factor, level)\n",
    "        cpt_cost += cost\n",
    "\n",
    "    return prc_cost, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b962e0f",
   "metadata": {},
   "source": [
    "## 3. Compaction Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fb69b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prs_cost(chunk_size, chunk_factor, level):\n",
    "    cost = 0\n",
    "    next_sizes = [chunk_size]\n",
    "\n",
    "    for _ in range(level):\n",
    "        cost += k_pcs_fixed_cost * len(next_sizes) + np.sum(next_sizes) * k_pcs_per_tuple_cost\n",
    "        next_sizes = np.repeat(np.array(next_sizes) // chunk_factor, chunk_factor)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Compute the cost of a single compaction\n",
    "def compute_cpt_cost(sizes_in_one_compaction):\n",
    "    return k_cpt_fixed_cost + np.sum(sizes_in_one_compaction) * k_cpt_per_tuple_cost\n",
    "\n",
    "\n",
    "# Strategy 1: Do not compact any chunks\n",
    "def alg_no_compaction(chunk_sizes, chunk_factor, level):\n",
    "    return chunk_sizes, 0\n",
    "\n",
    "\n",
    "# Strategy 2: Fully compact all chunks\n",
    "def alg_full_compaction(chunk_sizes, chunk_factor, level):\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "    cpt_sizes = []\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        if size == 2048: \n",
    "            transformed_sizes.append(size)\n",
    "            continue\n",
    "\n",
    "        if sum(cpt_sizes) + size <= 2048:\n",
    "            cpt_sizes.append(size)\n",
    "        else:\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "            cpt_sizes = [size]\n",
    "    \n",
    "    if cpt_sizes:\n",
    "        cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "        transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0030d",
   "metadata": {},
   "source": [
    "### Optimal Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e214a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: sort all chunks ascendingly and compact them in order, until compaction is not beneficial\n",
    "def alg_sort_compaction(chunk_sizes, chunk_factor, level):\n",
    "    sorted_sizes = sorted(chunk_sizes)\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "    cpt_sizes = []\n",
    "\n",
    "    i = 0\n",
    "    for i in range(len(sorted_sizes)):\n",
    "        size = sorted_sizes[i]\n",
    "        if sum(cpt_sizes) + size <= 2048:\n",
    "            gain = compute_prs_cost(sum(cpt_sizes), chunk_factor, level) + compute_prs_cost(cpt_sizes, chunk_factor, level) - compute_prs_cost(sum(cpt_sizes) + size, chunk_factor, level)\n",
    "            gain -= k_cpt_fixed_cost + size * k_cpt_per_tuple_cost\n",
    "            if gain > 0:\n",
    "                cpt_sizes.append(size)\n",
    "            else:\n",
    "                if len(cpt_sizes) > 1:\n",
    "                    cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "                transformed_sizes.append(sum(cpt_sizes))\n",
    "                break\n",
    "        else:\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "            cpt_sizes = [size]\n",
    "\n",
    "    for j in range(i, len(sorted_sizes)):\n",
    "        transformed_sizes.append(sorted_sizes[j])\n",
    "\n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "87205e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[No Compaction]\u001b[0m\n",
      "\t Total Cost: 2.85\tCompute Cost: 2.85s\t Compaction Cost: 0.00s\n",
      "\u001b[32m[Full Compaction]\u001b[0m\n",
      "\t Total Cost: 3.64\tCompute Cost: 2.73s\t Compaction Cost: 0.90s\n",
      "\u001b[32m[Sort Compaction]\u001b[0m\n",
      "\t Total Cost: 3.06\tCompute Cost: 2.75s\t Compaction Cost: 0.30s\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = generate_chunk_sizes(n=int(1e7 / 2048), mean=2048, scale=0)\n",
    "chunk_factor = 3\n",
    "level = 3\n",
    "\n",
    "grades = {\n",
    "    \"No Compaction\": simulate_join(chunk_sizes, alg_no_compaction, chunk_factor, level), \n",
    "    \"Full Compaction\": simulate_join(chunk_sizes, alg_full_compaction, chunk_factor, level),\n",
    "    \"Sort Compaction\": simulate_join(chunk_sizes, alg_sort_compaction, chunk_factor, level),\n",
    "}\n",
    "\n",
    "for grade in grades:\n",
    "    prc_cost = grades[grade][0]/1e6\n",
    "    cpt_cost = grades[grade][1]/1e6\n",
    "    print_color(f\"[{grade}]\", 'green')\n",
    "    print(f\"\\t Total Cost: {prc_cost + cpt_cost:.2f}\\tCompute Cost: {prc_cost:.2f}s\\t Compaction Cost: {cpt_cost:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cab200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
