{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374c044918092d9c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compaction Project\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Given a set of chunks whose sizes are\n",
    "\n",
    "$$\n",
    "S_n = \\{d_1, \\cdots, d_n\\},\n",
    "$$\n",
    "\n",
    "where the positive integer $d_i \\leq 2048$ for all $i = 1, \\cdots, n$. Suppose all join operators need time $f(d_i)$ to process a data chunk with the size $d_i$. \n",
    "\n",
    "Our goal is to compact the set $S$, i.e., we need a transformation\n",
    "\n",
    "$$\n",
    "\\mathcal{M}: S_n \\rightarrow S'_m \\triangleq \\{d'_1,  \\cdots, d'_m\\},\n",
    "$$\n",
    "\n",
    "where $\\sum_i^n d_i = \\sum_j^m d'_j$ and $m$ is an arbitrary integer less than $n$, to minimize \n",
    "\n",
    "$$\n",
    "\\sum_j^m f(d'_j) + cost(M, S).\n",
    "$$\n",
    "\n",
    "where $cost(\\mathcal{M}, S)$ is the cost of the transformation $\\mathcal{M}$ on the set $S$. The cost of combining two or more chunks into one: $d_i + \\cdots + d_j = d'_s \\leq 2048$, is \n",
    "\n",
    "$$\n",
    "g(d'_s) = C_3 + d'_s \\times C_4.\n",
    "$$\n",
    "\n",
    "**Note:** This formulated problem is easier than the real compaction problem because we have the sizes of all chunks in advance, rather than a chunk stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9897d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from termcolor import colored\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# settings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def print_color(text, color='black'):\n",
    "    print(colored(text, color))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50c30d",
   "metadata": {},
   "source": [
    "## 1. Chunk Sizes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f7c1da65ee0c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate random chunk sizes from a Gaussian distribution\n",
    "def generate_chunk_sizes(n, mean=64, scale=256):\n",
    "    return np.minimum(2048, np.maximum(1, np.random.normal(mean, scale, n))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e3e7b",
   "metadata": {},
   "source": [
    "## 2. Compaction Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             fixed cost      per tuple cost\n",
    "# probe()     1.5             0.03\n",
    "# next()      0.9             0.06\n",
    "# --------------------------------------\n",
    "# compact()   0.3             0.03\n",
    "# --------------------------------------\n",
    "\n",
    "k_prc_fixed_cost = (1.5 + 0.9)\n",
    "k_prc_per_tuple_cost = (0.03 + 0.06)\n",
    "k_cpt_fixed_cost = 0.3\n",
    "k_cpt_per_tuple_cost = 0.03\n",
    "\n",
    "# This function split each chunk into smaller chunks.\n",
    "def split_array_np(numbers, parts):\n",
    "    numbers = np.array(numbers)\n",
    "\n",
    "    quotient, remainder = np.divmod(numbers, parts)\n",
    "    split = np.repeat(quotient, parts)\n",
    "    remainder = np.repeat(remainder, parts)\n",
    "    split[np.arange(numbers.size * parts) % parts < remainder] += 1\n",
    "        \n",
    "    split = split[split > 0]\n",
    "\n",
    "    return split\n",
    "\n",
    "# Simulate the execution of joins.\n",
    "def simulate_join(sizes, compact_func, chunk_factor=1, n_join=1, print_log=True):\n",
    "    prc_cost = 0\n",
    "    cpt_cost = 0\n",
    "    next_sizes = np.array(sizes)\n",
    "\n",
    "    if print_log:\n",
    "        print_color(f\"-------------------------\", 'green')\n",
    "        print_color(f\"Compactor {compact_func}\", 'green')\n",
    "\n",
    "    for l in reversed(range(n_join)):\n",
    "        # 1. Join\n",
    "        prc_cost += k_prc_fixed_cost * len(next_sizes) + np.sum(next_sizes) * k_prc_per_tuple_cost\n",
    "\n",
    "        # 2. Split chunks\n",
    "        next_sizes = split_array_np(next_sizes, chunk_factor)\n",
    "        input_chunk_number = len(next_sizes)\n",
    "\n",
    "        # 3. Compact\n",
    "        next_sizes, cost = compact_func(next_sizes, chunk_factor, l)\n",
    "        output_chunk_number = len(next_sizes)\n",
    "\n",
    "        if print_log:\n",
    "            print_color(f\"Level {n_join - l}: {input_chunk_number} -> {output_chunk_number} chunks, cost: {cost:.2f}\", 'green')\n",
    "\n",
    "        cpt_cost += cost\n",
    "\n",
    "    return prc_cost, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b962e0f",
   "metadata": {},
   "source": [
    "## 3. Compaction Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afe82a",
   "metadata": {},
   "source": [
    "### Cost Calculation\n",
    "\n",
    "Suppose the $i$-th join operator needs time $f_1(d) = C_1 + C_2 \\cdot d$ to process a data chunk with the size $d$. \n",
    "\n",
    "We also suppose the pipeline has $n$ join operators, each join receives a data chunk, and outputs $m$ smaller chunks. \n",
    "\n",
    "Note that the total number of tuples across these smaller chunks remains the same as the number of tuples in the original input chunk. \n",
    "\n",
    "Then, for an input chunk with the size $d$, the time needed to process consists of two parts: \n",
    "\n",
    "1. Per Tuple Cost \n",
    "\n",
    "$$ C_1 \\cdot d \\cdot n $$ \n",
    "\n",
    "2. Fixed Cost\n",
    "\n",
    "$$ C_2 \\cdot \\min\\{m^0, d\\} + C_2 \\cdot \\min\\{m^1, d\\} + \\cdots + C_2 \\cdot \\min\\{m^{n-1}, d\\}$$\n",
    "\n",
    "We need to take the minimum value betwwen $m^{i}$ and the $d$ because a data chunk cannot be split into more than $d$ smaller chunks. In other words, each chunk has at least one tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prc_cost(chunk_size, chunk_factor, n_joins):\n",
    "    per_tuple_cost = k_prc_per_tuple_cost * chunk_size * n_joins\n",
    "\n",
    "    fixed_cost = 0\n",
    "    for i in range(n_joins):\n",
    "        fixed_cost += k_prc_fixed_cost * np.minimum(chunk_factor ** i, chunk_size)\n",
    "    \n",
    "    # print(f\"per_tuple_cost: {per_tuple_cost:.2f}, fixed_cost: {fixed_cost:.2f}\")\n",
    "    return per_tuple_cost + fixed_cost\n",
    "\n",
    "\n",
    "# Compute the cost of a single compaction\n",
    "def compute_cpt_cost(sizes_in_one_compaction):\n",
    "    return k_cpt_fixed_cost + np.sum(sizes_in_one_compaction) * k_cpt_per_tuple_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b05aa4",
   "metadata": {},
   "source": [
    "### Base Strategies\n",
    "\n",
    "**Strategy 1**: Do not compact any chunks. \n",
    "\n",
    "**Strategy 2**: Fully compact all chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_no_compaction(chunk_sizes, chunk_factor, level):\n",
    "    return chunk_sizes, 0\n",
    "\n",
    "\n",
    "def alg_full_compaction(chunk_sizes, chunk_factor, level):\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "    cpt_sizes = []\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        if size == 2048: \n",
    "            transformed_sizes.append(size)\n",
    "            continue\n",
    "\n",
    "        if sum(cpt_sizes) + size <= 2048:\n",
    "            cpt_sizes.append(size)\n",
    "        else:\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "            cpt_sizes = [size]\n",
    "    \n",
    "    if cpt_sizes:\n",
    "        cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "        transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0030d",
   "metadata": {},
   "source": [
    "### Optimal Strategy\n",
    "\n",
    "**Strategy 3**: Sort all chunks ascendingly and compact them in order, until compaction is not beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_sort_compaction(chunk_sizes, chunk_factor, n_joins):\n",
    "    assert len(chunk_sizes) > 0, \"chunk_sizes must not be empty\"\n",
    "\n",
    "    sorted_sizes = sorted(chunk_sizes)\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "\n",
    "    i = 0\n",
    "    cpt_sizes = [sorted_sizes[0]]\n",
    "    for i in range(1, len(sorted_sizes), 1):\n",
    "        size = sorted_sizes[i]\n",
    "        cur_sum = sum(cpt_sizes)\n",
    "\n",
    "        if cur_sum + size <= 2048:\n",
    "            gain = compute_prc_cost(cur_sum, chunk_factor, n_joins) + compute_prc_cost(size, chunk_factor, n_joins) - compute_prc_cost(cur_sum + size, chunk_factor, n_joins)\n",
    "            loss = (k_cpt_fixed_cost + size * k_cpt_per_tuple_cost)\n",
    "            \n",
    "            # print(f\"gain: {gain:.2f}, loss: {loss:.2f}\")\n",
    "            # print(f\"cur sum {cur_sum} + size {size} = {cur_sum + size}\")\n",
    "\n",
    "            if gain - loss > 0:\n",
    "                cpt_sizes.append(size)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            if len(cpt_sizes) > 1:\n",
    "                cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "            cpt_sizes = [size]\n",
    "\n",
    "    if cpt_sizes:\n",
    "        if len(cpt_sizes) > 1:\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "        transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "    # Append the remaining chunks, it is not beneficial to compact them\n",
    "    for j in range(i+1, len(sorted_sizes)):\n",
    "        transformed_sizes.append(sorted_sizes[j])\n",
    "        \n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9407bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_benefit(buffer, target, chunk_factor, n_joins):\n",
    "    prc_gain = compute_prc_cost(buffer, chunk_factor, n_joins) + compute_prc_cost(target, chunk_factor, n_joins) - compute_prc_cost(buffer + target, chunk_factor, n_joins)\n",
    "    cpt_cost = (k_cpt_fixed_cost + target * k_cpt_per_tuple_cost)\n",
    "    return prc_gain - cpt_cost\n",
    "\n",
    "chunk_factor = 4\n",
    "n_joins = 2\n",
    "x_base = np.arange(1, 2048, 1)\n",
    "y_add = np.arange(1, 2048, 1)\n",
    "\n",
    "z_benefits = np.zeros((2047, 2047))\n",
    "for i in range(2047):\n",
    "    for j in range(2047):\n",
    "        if x_base[i] + y_add[j] <= 2048 and x_base[i] + y_add[j] > 0:\n",
    "            z_benefits[j][i] = compute_benefit(x_base[i], y_add[j], chunk_factor, n_joins)\n",
    "        else:\n",
    "            z_benefits[j][i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming z_benefits is a numpy array\n",
    "y_zero, x_zero = np.where(np.abs(z_benefits) < 0.01)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(x=x_base, y=y_add, z=z_benefits, colorscale='ice', showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_base[x_zero], y=y_add[y_zero], mode='lines', name='z = 0', line=dict(color='black',width=2)))\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=500,\n",
    "    height=500,\n",
    "    title=\"Compaction Benefit\",\n",
    "    xaxis_title=\"Buffer Chunk Size\",\n",
    "    yaxis_title=\"Target Chunk Size\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1444548",
   "metadata": {},
   "source": [
    "### DuckDB Strategy\n",
    "\n",
    "**Strategy 4**: Set a threhold to distinguish the big chunk and the small chunk, we only compact small chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_block_size = 2048\n",
    "k_duckdb_compaction_threshold = 128\n",
    "\n",
    "def alg_binary_compaction(chunk_sizes, chunk_factor, level):\n",
    "    trans_chunks = []\n",
    "    cpt_cost = 0\n",
    "    \n",
    "    cpt_chunks = []\n",
    "    for size in chunk_sizes:\n",
    "        if size >= k_duckdb_compaction_threshold:\n",
    "            trans_chunks.append(size)\n",
    "            continue\n",
    "\n",
    "        cpt_chunks.append(size)\n",
    "\n",
    "        if sum(cpt_chunks) >= k_block_size - k_duckdb_compaction_threshold:\n",
    "            trans_chunks.append(np.sum(cpt_chunks))\n",
    "            cpt_cost += compute_cpt_cost(cpt_chunks)\n",
    "            cpt_chunks = []\n",
    "    \n",
    "    return trans_chunks, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb8b97",
   "metadata": {},
   "source": [
    "**Strategy 5**: The last strategy uses fixed threshold 128 for all levels. It is not precise, and we can compute a better one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpt_threhold(chunk_factor, n_joins):\n",
    "    if n_joins == 0:\n",
    "        return 0\n",
    "\n",
    "    x = 1024\n",
    "    y = np.arange(1, 2048, 1)\n",
    "    z = np.zeros((2047))\n",
    "\n",
    "    for i in range(2047):\n",
    "        if x + y[i] <= 2048 and x + y[i] > 0:\n",
    "            z[i] = compute_benefit(x, y[i], chunk_factor, n_joins)\n",
    "        else:\n",
    "            z[i] = None\n",
    "\n",
    "    y_zero = np.where(np.abs(z) < 0.01)\n",
    "\n",
    "    return y_zero[0][0] if len(y_zero[0]) > 0 else 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_dynamic_compaction(chunk_sizes, chunk_factor, level):\n",
    "    cpt_threshold = get_cpt_threhold(chunk_factor, level)\n",
    "\n",
    "    print_color(f\"Compaction Threshold: {cpt_threshold}\", 'blue')\n",
    "\n",
    "    trans_chunks = []\n",
    "    cpt_cost = 0\n",
    "    \n",
    "    cpt_chunks = []\n",
    "    for size in chunk_sizes:\n",
    "        if size >= cpt_threshold:\n",
    "            trans_chunks.append(size)\n",
    "            continue\n",
    "\n",
    "        cpt_chunks.append(size)\n",
    "\n",
    "        if sum(cpt_chunks) >= k_block_size - cpt_threshold:\n",
    "            trans_chunks.append(np.sum(cpt_chunks))\n",
    "            cpt_cost += compute_cpt_cost(cpt_chunks)\n",
    "            cpt_chunks = []\n",
    "\n",
    "    \n",
    "    return trans_chunks, cpt_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e815e7",
   "metadata": {},
   "source": [
    "## 4. Let's Join!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f51c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def let_us_join(chunk_sizes, chunk_factor, num_join, print_log=True):\n",
    "    grades = {\n",
    "        \"No Compaction\": simulate_join(chunk_sizes, alg_no_compaction, chunk_factor, num_join, print_log), \n",
    "        \"Full Compaction\": simulate_join(chunk_sizes, alg_full_compaction, chunk_factor, num_join, print_log),\n",
    "        \"Sort Compaction\": simulate_join(chunk_sizes, alg_sort_compaction, chunk_factor, num_join, print_log),\n",
    "        \"Binary Compaction\": simulate_join(chunk_sizes, alg_binary_compaction, chunk_factor, num_join, print_log), \n",
    "        \"Dynamic Compaction\": simulate_join(chunk_sizes, alg_dynamic_compaction, chunk_factor, num_join, print_log),\n",
    "    }\n",
    "\n",
    "    print_color(f\"-------------------------\", 'green')\n",
    "\n",
    "    for grade in grades:\n",
    "        prc_cost = grades[grade][0]/1e6\n",
    "        cpt_cost = grades[grade][1]/1e6\n",
    "        print_color(f\"[{grade}]\", 'green')\n",
    "        print(f\"\\t Total Cost: {prc_cost + cpt_cost:.2f}s\\tCompute Cost: {prc_cost:.2f}s\\t Compaction Cost: {cpt_cost:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87205e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCompaction Threshold: 1024\u001b[0m\n",
      "\u001b[34mCompaction Threshold: 69\u001b[0m\n",
      "\u001b[34mCompaction Threshold: 0\u001b[0m\n",
      "\u001b[32m-------------------------\u001b[0m\n",
      "\u001b[32m[No Compaction]\u001b[0m\n",
      "\t Total Cost: 11.66s\tCompute Cost: 11.66s\t Compaction Cost: 0.00s\n",
      "\u001b[32m[Full Compaction]\u001b[0m\n",
      "\t Total Cost: 7.09s\tCompute Cost: 5.33s\t Compaction Cost: 1.76s\n",
      "\u001b[32m[Sort Compaction]\u001b[0m\n",
      "\t Total Cost: 6.27s\tCompute Cost: 5.68s\t Compaction Cost: 0.59s\n",
      "\u001b[32m[Binary Compaction]\u001b[0m\n",
      "\t Total Cost: 6.96s\tCompute Cost: 5.52s\t Compaction Cost: 1.45s\n",
      "\u001b[32m[Dynamic Compaction]\u001b[0m\n",
      "\t Total Cost: 6.55s\tCompute Cost: 5.62s\t Compaction Cost: 0.94s\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = generate_chunk_sizes(n=int(2e7 / 2048), mean=2048, scale=128)\n",
    "chunk_factor = 16\n",
    "num_join = 3\n",
    "print_log = False\n",
    "\n",
    "let_us_join(chunk_sizes, chunk_factor, num_join, print_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f69ace7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCompaction Threshold: 134\u001b[0m\n",
      "\u001b[34mCompaction Threshold: 6\u001b[0m\n",
      "\u001b[34mCompaction Threshold: 0\u001b[0m\n",
      "\u001b[32m-------------------------\u001b[0m\n",
      "\u001b[32m[No Compaction]\u001b[0m\n",
      "\t Total Cost: 11.66s\tCompute Cost: 11.66s\t Compaction Cost: 0.00s\n",
      "\u001b[32m[Full Compaction]\u001b[0m\n",
      "\t Total Cost: 22.89s\tCompute Cost: 5.33s\t Compaction Cost: 17.56s\n",
      "\u001b[32m[Sort Compaction]\u001b[0m\n",
      "\t Total Cost: 11.53s\tCompute Cost: 5.68s\t Compaction Cost: 5.85s\n",
      "\u001b[32m[Binary Compaction]\u001b[0m\n",
      "\t Total Cost: 19.89s\tCompute Cost: 5.52s\t Compaction Cost: 14.36s\n",
      "\u001b[32m[Dynamic Compaction]\u001b[0m\n",
      "\t Total Cost: 11.54s\tCompute Cost: 5.69s\t Compaction Cost: 5.85s\n"
     ]
    }
   ],
   "source": [
    "k_cpt_per_tuple_cost *= 10\n",
    "chunk_sizes = generate_chunk_sizes(n=int(2e7 / 2048), mean=2048, scale=128)\n",
    "chunk_factor = 16\n",
    "num_join = 3\n",
    "print_log = False\n",
    "\n",
    "let_us_join(chunk_sizes, chunk_factor, num_join, print_log)\n",
    "\n",
    "k_cpt_per_tuple_cost /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d71d5ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCompaction Threshold: 1024\u001b[0m\n",
      "\u001b[34mCompaction Threshold: 134\u001b[0m\n",
      "\u001b[34mCompaction Threshold: 6\u001b[0m\n",
      "\u001b[34mCompaction Threshold: 0\u001b[0m\n",
      "\u001b[32m-------------------------\u001b[0m\n",
      "\u001b[32m[No Compaction]\u001b[0m\n",
      "\t Total Cost: 60.20s\tCompute Cost: 60.20s\t Compaction Cost: 0.00s\n",
      "\u001b[32m[Full Compaction]\u001b[0m\n",
      "\t Total Cost: 30.51s\tCompute Cost: 7.11s\t Compaction Cost: 23.40s\n",
      "\u001b[32m[Sort Compaction]\u001b[0m\n",
      "\t Total Cost: 19.16s\tCompute Cost: 7.46s\t Compaction Cost: 11.70s\n",
      "\u001b[32m[Binary Compaction]\u001b[0m\n",
      "\t Total Cost: 27.53s\tCompute Cost: 7.30s\t Compaction Cost: 20.23s\n",
      "\u001b[32m[Dynamic Compaction]\u001b[0m\n",
      "\t Total Cost: 19.20s\tCompute Cost: 7.49s\t Compaction Cost: 11.70s\n"
     ]
    }
   ],
   "source": [
    "k_cpt_per_tuple_cost *= 10\n",
    "chunk_sizes = generate_chunk_sizes(n=int(2e7 / 2048), mean=2048, scale=128)\n",
    "chunk_factor = 16\n",
    "num_join = 4\n",
    "print_log = False\n",
    "\n",
    "let_us_join(chunk_sizes, chunk_factor, num_join, print_log)\n",
    "\n",
    "k_cpt_per_tuple_cost /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f41bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
