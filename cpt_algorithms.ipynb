{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374c044918092d9c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Compaction Project\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Given a set of chunks whose sizes are\n",
    "\n",
    "$$\n",
    "S_n = \\{d_1, \\cdots, d_n\\},\n",
    "$$\n",
    "\n",
    "where the positive integer $d_i \\leq 2048$ for all $i = 1, \\cdots, n$. Suppose the remaining operators need time \n",
    "\n",
    "$$\n",
    "f(d_i) = C_1 + d_i \\times C_2\n",
    "$$\n",
    "\n",
    "to process a data chunk with the size $d_i$. Our goal is to compact the set $S$, i.e., we need a transformation\n",
    "\n",
    "$$\n",
    "\\mathcal{M}: S_n \\rightarrow S'_m \\triangleq \\{d'_1,  \\cdots, d'_m\\},\n",
    "$$\n",
    "\n",
    "where $\\sum_i^n d_i = \\sum_j^m d'_j$ and $m$ is an arbitrary integer less than $n$, to minimize \n",
    "\n",
    "$$\n",
    "\\sum_j^m f(d'_j) + cost(M, S).\n",
    "$$\n",
    "\n",
    "where $cost(\\mathcal{M}, S)$ is the cost of the transformation $\\mathcal{M}$ on the set $S$. \n",
    "\n",
    "The cost of combining two or more chunks into one: $d_i + \\cdots + d_j = d'_s \\leq 2048$, is \n",
    "\n",
    "$$\n",
    "g(d'_s) = C_3 + d'_s \\times C_4.\n",
    "$$\n",
    "\n",
    "**Note:** This formulated problem is easier than the real compaction problem because we have the sizes of all data chunks in advance, rather than a chunk stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1ae6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from termcolor import colored\n",
    "\n",
    "def print_color(text, color='black'):\n",
    "    print(colored(text, color))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50c30d",
   "metadata": {},
   "source": [
    "## 1. Distribution of Chunk Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c62f7c1da65ee0c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random chunk sizes from a Gaussian distribution\n",
    "def generate_chunk_sizes(n, mean=64, scale=256):\n",
    "    return np.maximum(0, np.random.normal(mean, scale, n)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e3e7b",
   "metadata": {},
   "source": [
    "## 2. Define the Processing Cost and Compaction Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30b8dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             fixed cost      per tuple cost\n",
    "# probe()     1.5             0.03\n",
    "# next()      0.9             0.06\n",
    "# --------------------------------------\n",
    "# compact()   0.3             0.03\n",
    "# --------------------------------------\n",
    "\n",
    "k_pcs_fixed_cost = (1.5 + 0.9)\n",
    "k_pcs_per_tuple_cost = (0.03 + 0.06)\n",
    "k_cpt_fixed_cost = 0.3\n",
    "k_cpt_per_tuple_cost = 0.03\n",
    "\n",
    "def simulate_join(sizes, compact_func, chunk_factor=1, level=1):\n",
    "    prc_cost = 0\n",
    "    cpt_cost = 0\n",
    "    next_sizes = np.array(sizes)\n",
    "\n",
    "    for _ in range(level):\n",
    "        # join\n",
    "        prc_cost += k_pcs_fixed_cost * len(next_sizes) + np.sum(next_sizes) * k_pcs_per_tuple_cost\n",
    "        next_sizes = np.repeat(next_sizes // chunk_factor, chunk_factor)\n",
    "\n",
    "        # compact\n",
    "        next_sizes, cost = compact_func(next_sizes)\n",
    "        cpt_cost += cost\n",
    "\n",
    "    return prc_cost, cpt_cost\n",
    "\n",
    "\n",
    "def compute_cpt_cost(compacted_chunk_sizes):\n",
    "    return k_cpt_fixed_cost + np.sum(compacted_chunk_sizes) * k_cpt_per_tuple_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b962e0f",
   "metadata": {},
   "source": [
    "## 3. Compaction Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fb69b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Do not compact any chunks\n",
    "def alg_no_compaction(chunk_sizes):\n",
    "    return chunk_sizes, 0\n",
    "\n",
    "# Strategy 2: Fully compact all chunks\n",
    "def alg_full_compaction(chunk_sizes):\n",
    "    transformed_sizes = []\n",
    "    cpt_cost = 0\n",
    "    cpt_sizes = []\n",
    "    for size in chunk_sizes:\n",
    "        if sum(cpt_sizes) + size <= 2048:\n",
    "            cpt_sizes.append(size)\n",
    "        else:\n",
    "            cpt_cost += compute_cpt_cost(cpt_sizes)\n",
    "            transformed_sizes.append(sum(cpt_sizes))\n",
    "            cpt_sizes = [size]\n",
    "    transformed_sizes.append(sum(cpt_sizes))\n",
    "\n",
    "    return transformed_sizes, cpt_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "87205e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mNo Compaction cost: 52408.32 microseconds\u001b[0m\n",
      "\u001b[32mFull Compaction cost: 60249.30 microseconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = generate_chunk_sizes(n=128, mean=2048, scale=0)\n",
    "chunk_factor = 8\n",
    "level = 2\n",
    "\n",
    "grades = {\n",
    "    \"No Compaction\": simulate_no_compaction(chunk_sizes, chunk_factor, level), \n",
    "    \"Full Compaction\": simulate_full_compaction(chunk_sizes, chunk_factor, level)\n",
    "}\n",
    "\n",
    "for grade in grades:\n",
    "    print_color(f\"{grade} cost: {grades[grade]:.2f} microseconds\", 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cab6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
